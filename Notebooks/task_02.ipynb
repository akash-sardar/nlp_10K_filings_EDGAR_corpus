{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3e1a7e",
   "metadata": {},
   "source": [
    "### TASK_01\n",
    "#### Coding Assignment Task # 1\n",
    "- Dataset: IEDGAR SEC filings (public data) -\n",
    "- https://huggingface.co/datasets/eloukas/edgar-corpus\n",
    "- Language: Python\n",
    "- Implementation: Pyspark\n",
    "- Submission: Github repository containing code + plots (jpeg).\n",
    "- Expected Maximum Duration: 3 hours\n",
    "\n",
    "Given a set of documents: create a solution that allows the end user to understand the documents in a two dimensional space and to identify outliers.\n",
    "Task #2 – Gen AI\n",
    "\n",
    "Dataset\n",
    "\n",
    "( Year: 2018-2020\n",
    "\n",
    "( Filing type: 10K\n",
    "\n",
    "( Sections: All\n",
    "\n",
    "( Company: Choose 1.\n",
    "\n",
    "( Choose 5 data attributes to extract from a single year.\n",
    "\n",
    "Steps\n",
    "\n",
    "( Convert documents to chunks\n",
    "\n",
    "( Covert chunks to embeddings\n",
    "\n",
    "( Create a query\n",
    "\n",
    "( Create a prompt to extract data from chunks from a specific year.\n",
    "\n",
    "( Create a validation dataset (5 true values from chunks).\n",
    "\n",
    "( Demonstrate that your LLM can retrieve the correct chunks from your\n",
    "\n",
    "embedding object for the correct year\n",
    "\n",
    "#### Dataset card:\n",
    "This dataset card is based on the paper EDGAR-CORPUS: Billions of Tokens Make The World Go Round authored by Lefteris Loukas et.al, as published in the ECONLP 2021 workshop.\n",
    "This dataset contains the annual reports of public companies from 1993-2020 from SEC EDGAR filings.\n",
    "There is supported functionality to load a specific year.\n",
    "Care: since this is a corpus dataset, different train/val/test splits do not have any special meaning. It's the default HF card format to have train/val/test splits.\n",
    "If you wish to load specific year(s) of specific companies, you probably want to use the open-source software which generated this dataset, EDGAR-CRAWLER: https://github.com/nlpaueb/edgar-crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed12efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "D:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>div.output_scroll { height: 44em; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import os\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import colorcet as cc\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler as sklearnScaler\n",
    "from sklearn.decomposition  import PCA as sklearnPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, col, count, when, length, concat_ws, \\\n",
    "                udf, explode, pandas_udf, lit, count, avg, max as spark_max, min as spark_min, length, expr\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType,  StructType, StructField, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load environnt variables from .env file\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'D:\\Softwares\\Microsoft\\jdk-17.0.15.6-hotspot'\n",
    "\n",
    "# import sys\n",
    "\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 44em; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7bc383-7d9e-4f06-a54c-1e1fb426b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cc043c-a172-4d9b-a832-280953e50e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd9c778-d2c6-4bc6-8e88-c18c793e0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AIG_SEC_Filing_Analysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.python.worker.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\",\"true\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\").getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"AIG_SEC_Filing_Analysis\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2315f47-ab1a-49b7-8d72-92de8d941087",
   "metadata": {},
   "source": [
    "### Based on the observations above :\n",
    "- The text will retain context if split with \"\\n\"\n",
    "- It might also help to consider \":\" as that sometime is identifying the start of a sub-section [ Text between \"\\n\" and \":\" will be the header]\n",
    "- \";\" might signal the end of a contextually similar section\n",
    "- Size of each chunk should be less than 512 tokens or about 2000 characters (to begin with) as I am planning to use BERT based sentence transformers\n",
    "- An overlap startegy of 200 characters to preserve context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c40f7ff-55d0-4a04-a832-700562fe2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_chunks(text:str, overlap: int = 200, chunk_size = 2000)-> list:\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r'[\\x00-\\x09\\x0B-\\x1F\\x7F-\\x9F]','', text)\n",
    "    chunks = []\n",
    "\n",
    "    \n",
    "    if len(text) <= chunk_size:\n",
    "        return [text.strip()] if text.strip() else []\n",
    "        \n",
    "    prev_chunk_len = 0  \n",
    "    \n",
    "    while len(text) > chunk_size:\n",
    "        right = chunk_size\n",
    "        \n",
    "        last_newline_index = text[:chunk_size].rfind(\"\\n\")\n",
    "        \n",
    "        if last_newline_index > 0:\n",
    "            right = last_newline_index\n",
    "\n",
    "        # to ensure chunks have complete words instead of cut words\n",
    "        # if right > 0 and text[right-1].isalnum() and text[right].isalnum():\n",
    "        #     last_space = text[:right].rfind(' ')\n",
    "        #     if last_space > right-50 and last_space > 0:\n",
    "        #         right = last_space\n",
    "            \n",
    "        chunk = text[:right].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if len(chunks) > 1 and prev_chunk_len < 1000 and len(chunks[-1]) < 1000 :\n",
    "            chunks[-2]+=\" \" + chunks[-1]\n",
    "            chunks = chunks[:-1]\n",
    "        \n",
    "        prev_chunk_len = len(chunks[-1])\n",
    "        \n",
    "        if right > overlap:\n",
    "            start = right - overlap\n",
    "            # Prevent index error\n",
    "            while start < len(text):\n",
    "                if start == 0:\n",
    "                    break\n",
    "                if not (text[start].isalnum() and text[start-1].isalnum()):\n",
    "                    break\n",
    "                start += 1\n",
    "            text = text[start:]\n",
    "        else:\n",
    "            text = text[right:]\n",
    "            \n",
    "    if text:\n",
    "        rem = text.strip()\n",
    "        if rem:\n",
    "            chunks.append(rem)        \n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8710c4-0c60-471c-a566-2c716eb69a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_companies = spark.read.parquet(\"D:\\\\PracticeProjects\\\\TASK_01\\\\Data\\\\data.parquet\")\n",
    "df = df_all_companies.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcb2291-14cd-45be-906b-56b1844dc8ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- cik: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- section_1: string (nullable = true)\n",
      " |-- section_1A: string (nullable = true)\n",
      " |-- section_1B: string (nullable = true)\n",
      " |-- section_2: string (nullable = true)\n",
      " |-- section_3: string (nullable = true)\n",
      " |-- section_4: string (nullable = true)\n",
      " |-- section_5: string (nullable = true)\n",
      " |-- section_6: string (nullable = true)\n",
      " |-- section_7: string (nullable = true)\n",
      " |-- section_7A: string (nullable = true)\n",
      " |-- section_8: string (nullable = true)\n",
      " |-- section_9: string (nullable = true)\n",
      " |-- section_9A: string (nullable = true)\n",
      " |-- section_9B: string (nullable = true)\n",
      " |-- section_10: string (nullable = true)\n",
      " |-- section_11: string (nullable = true)\n",
      " |-- section_12: string (nullable = true)\n",
      " |-- section_13: string (nullable = true)\n",
      " |-- section_14: string (nullable = true)\n",
      " |-- section_15: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b81829-9b86-48e7-bed8-0f45be8afb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f039689-9205-4e34-985e-89824ec14ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking_udf = udf(create_custom_chunks, ArrayType(StringType()))       \n",
    "@pandas_udf(returnType = ArrayType(StringType()))\n",
    "def chunk_pandas_udf(series):\n",
    "    return series.apply(create_custom_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1b75d8-e96f-44c7-979f-e062261b8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [column for column in df.columns if column not in [\"filename\", \"cik\", \"year\"]]\n",
    "# for column in columns:\n",
    "#     df = df.withColumn(f\"{column}\"+f\"_chunked\", chunk_pandas_udf(col(column)))\n",
    "\n",
    "# for column in columns:\n",
    "#     df_chunked = df.select(\"cik\", \"filename\", explode(col(f\"{column}_chunked\")).alias(\"chunk\")).withColumn(\"section_name\", column))\n",
    "# # for column in columns:\n",
    "# #     df = df.withColumn(f\"{column}\"+f\"_chunked\", chunking_udf(col(column)))\n",
    "\n",
    "\n",
    "columns = [column for column in df.columns if column not in [\"filename\", \"cik\", \"year\"]]\n",
    "\n",
    "for column in columns:\n",
    "    df = df.withColumn(column, when(col(column).isNotNull(), col(column)).otherwise(lit(\"\")))\n",
    "# Create chunks\n",
    "for column in columns:\n",
    "    df = df.withColumn(f\"{column}_chunked\", chunk_pandas_udf(col(column)))\n",
    "\n",
    "# Explode and collect all sections\n",
    "all_chunks = []\n",
    "for column in columns:\n",
    "    df_section = df.select(\n",
    "        \"cik\",\n",
    "        \"filename\",\n",
    "        \"year\",\n",
    "        explode(col(f\"{column}_chunked\")).alias(\"chunk\")\n",
    "    ).withColumn(\"section_name\", lit(column)).withColumn(\"chunk_id\", expr(\"uuid()\"))\n",
    "    \n",
    "    all_chunks.append(df_section)\n",
    "\n",
    "# Union all\n",
    "df_exploded = all_chunks[0]\n",
    "for chunk_df in all_chunks[1:]:\n",
    "    df_exploded = df_exploded.union(chunk_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f078aa3a-bbcb-47be-81d1-75701d3dfd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cik', 'filename', 'year', 'chunk', 'section_name', 'chunk_id']\n"
     ]
    }
   ],
   "source": [
    "print(df_exploded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771e67be-9d7e-4f9a-b98c-5b8ced707b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_pd = df_exploded.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfd6933-5470-473a-9506-75baf88bc5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.84s/it]\n"
     ]
    }
   ],
   "source": [
    "texts = df_exploded_pd[\"chunk\"].tolist()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "embeddings = model.encode(\n",
    "    texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2de5dd4-2605-4c21-89e5-0a27eebc0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains NaNs? False\n",
      "Number of empty embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "has_nan = np.isnan(embeddings).any()\n",
    "print(f\"Contains NaNs? {has_nan}\")\n",
    "\n",
    "# Check if any embedding vectors are empty or zero vectors\n",
    "empty_vectors = np.sum(embeddings, axis=1) == 0\n",
    "print(f\"Number of empty embeddings: {np.sum(empty_vectors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb6570e0-a91b-4c6a-b9c9-724d64da1481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed6c706-e6c9-44ee-bc5b-b49054ee3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_pd[\"embeddings\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a779467-07e3-481f-b942-8e46fbe8e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = spark.createDataFrame(df_exploded_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f90cf1-3688-4ad7-aa11-a28b41871412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cik: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- chunk: string (nullable = true)\n",
      " |-- section_name: string (nullable = true)\n",
      " |-- chunk_id: string (nullable = true)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203cf29a-8623-4b62-8f85-4f6ae44acc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3bb0336-7ac4-49b6-9dc5-0b512440e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the bank compete with?\"\n",
    "extracted_year = \"2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a7d1d0-8770-4280-b318-c0bee0ddc3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = model.encode(query, convert_to_numpy = True)\n",
    "query_embedding = query_embedding.reshape(1,-1)\n",
    "query_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fa05876-db0c-4373-be51-071a15892d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_year(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return None\n",
    "#     entities = NER(text).ents\n",
    "#     year = [ent.text for ent in entities if ent.label_ == \"DATE\"]\n",
    "#     pattern = r'20\\d{2}|19\\d{2}'\n",
    "#     if year:\n",
    "#         extracted_year = re.findall(pattern, year[0])[0]\n",
    "#         return extracted_year\n",
    "#     else:\n",
    "#         return None\n",
    "# extracted_year = find_year(query)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8677bac-3a8e-4206-b628-44be766dca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if extracted_year:\n",
    "    filtered_embeddings_df = df_embeddings.filter(col(\"year\") == extracted_year)\n",
    "else:\n",
    "    filtered_embeddings_df = df_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "586187ce-8692-41b5-ab1b-5e554af41818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cik: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- chunk: string (nullable = true)\n",
      " |-- section_name: string (nullable = true)\n",
      " |-- chunk_id: string (nullable = true)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_embeddings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cc6973f-9060-4a06-b4fa-2324347ef25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_similarity(embedding1, embedding2):\n",
    "#     dot_product = sum(a * b for a,b in zip(embedding1, embedding2))\n",
    "#     norm1 = sqrt(sum(a * a for a in embedding1))\n",
    "#     norm2 = sqrt(sum(b * b for b in embedding2))\n",
    "#     return dot_product/(norm1 * norm2)\n",
    "\n",
    "# @pandas_udf(returnType = DoubleType())\n",
    "# def similarity_pandas_udf(embedding_series, query_embedding):\n",
    "#     query_embedding = query_broadcast.value\n",
    "#     return embedding_series.apply(cosine_similarity, query_embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03e883e2-cfe8-4d42-a716-39e611e0ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute query vector\n",
    "query_vector = Vectors.dense(query_embedding)\n",
    "\n",
    "@udf(DoubleType())\n",
    "def cosine_similarity_udf(vec):\n",
    "    # Convert list to DenseVector\n",
    "    if isinstance(vec, list):\n",
    "        vec = Vectors.dense(vec)\n",
    "    \n",
    "    dot = float(vec.dot(query_vector))\n",
    "    norm1 = float(vec.norm(2))\n",
    "    norm2 = float(query_vector.norm(2))\n",
    "    \n",
    "    return dot / (norm1 * norm2) if norm1 != 0 and norm2 != 0 else 0.0\n",
    "\n",
    "df_result = filtered_embeddings_df.withColumn(\"score\", cosine_similarity_udf(col(\"embeddings\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c838ed7-2e37-4372-8603-c4bac8afb5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cik: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- chunk: string (nullable = true)\n",
      " |-- section_name: string (nullable = true)\n",
      " |-- chunk_id: string (nullable = true)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52461b15-3b61-4503-95ff-db9f589626b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks_df = df_result.orderBy(col(\"score\").desc()).select(\"cik\",\"year\", \"section_name\",\"chunk\",\"chunk_id\").limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8169004a-2423-48a2-93b3-3f8e1a604622",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o860.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 50.0 failed 1 times, most recent failure: Lost task 10.0 in stage 50.0 (TID 321) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.writeNextInputToStream(PythonUDFRunner.scala:69)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:98)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\r\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:795)\r\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 57 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:318)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:316)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:312)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.writeNextInputToStream(PythonUDFRunner.scala:69)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:98)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\r\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:795)\r\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\t... 3 more\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 57 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m top_chunks_df.count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m._jdf.count())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*a, **kw)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Softwares\\Anaconda3\\envs\\coding_task_venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o860.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 50.0 failed 1 times, most recent failure: Lost task 10.0 in stage 50.0 (TID 321) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.writeNextInputToStream(PythonUDFRunner.scala:69)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:98)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\r\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:795)\r\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 57 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:318)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:316)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:312)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:333)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.writeNextInputToStream(PythonUDFRunner.scala:69)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:98)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\r\n\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:795)\r\n\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\t... 3 more\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:855)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\t... 57 more\r\n"
     ]
    }
   ],
   "source": [
    "top_chunks_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d2a6-5007-4810-a78e-b077c88d7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df33ef-1243-40ba-afe4-985838b6ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_context = top_chunks_df.collect()\n",
    "context = \"\\n\\n\".join([row[\"chunk\"] for row in top_k_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec14f4-d589-4e5c-9b86-b45357d14b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d916bb-ddf4-48e4-b8ff-13feb710c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"This is a SEC 10K filing context. As an expert financial analyst, extract information about the query {query}. \n",
    "Use the below context to answer with just the exact dollar amount. If answer is not found, reply - not found.\n",
    "Context: {context}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fbc08-364b-4225-b74b-4f27bb6d0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e548a6-1401-4f62-8f16-450cf8a15be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc58efa-37eb-4134-af86-e7a354fce0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "opeai_token = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58657c-73fb-4e63-8c3e-6c18b0ad9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21e3b9-c371-406e-b1be-c0a8a83d9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88998e-1288-4451-a8dd-a847129f8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ba6ee-ee42-4acd-b488-67d7287f1047",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "bf88f2ad-0799-4b1f-976f-4b2dc0c76d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = [\n",
    "    {\n",
    "        \"query\": \"What is the outstanding amount of Subordinated Debentures in 2020?\",\n",
    "        \"expected_value\": \"$12,887,000\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the aggregate market value of the voting stock held by non-affiliates in 2020 ?\",\n",
    "        \"expected_value\": \"$65,684,788\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much money was allocated under the HHSB Act in 2020 ?\",\n",
    "        \"expected_value\": \"$284.45 billion\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },    \n",
    "    {\n",
    "        \"query\": \"What is the FDIC’s standard maximum deposit insurance amount in 2020 ?\",\n",
    "        \"expected_value\": \"$250,000\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the aggregate market value of the voting stock held by non-affiliates in 2018 ?\",\n",
    "        \"expected_value\": \"$82,097,082\",\n",
    "        \"year\": \"2018\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What amount of wholesale deposit was purchased in 2018 ?\",\n",
    "        \"expected_value\": \"$35.3 million\",\n",
    "        \"year\": \"2018\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    }\n",
    "    {\n",
    "        \"query\": \"How much money was allocated under the HHSB Act in 2018 ?\",\n",
    "        \"expected_value\": \"not found\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much money was allocated under the CARES Act in 2018 ?\",\n",
    "        \"expected_value\": \"not found\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the total FDIC insurance assessment in 2019 ?\",\n",
    "        \"expected_value\": \"$64,079\",\n",
    "        \"year\": \"2020\",\n",
    "        \"chunk_id\": \"actual_chunk_id_here\",\n",
    "        \"section\": \"item_8\"\n",
    "    },\n",
    "]\n",
    "   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632eaf60-1a57-4dc3-9816-19b5101d3d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236f520-df0b-4d25-a221-d662b8dee480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
